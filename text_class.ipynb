{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Deep PLN para Análise de Sentimentos\n",
    "\n",
    "Análise de sentimento é um termo geral para se referir a tarefa de extrair emoções a partir de dados gerados por pessoas. Dentre esses dados, o mais comum para analise de sentimento é texto. A partir de comentários  nas mídias sociais, podemos ver como as pessoas reagem a uma postagem ou notícia, como estão avaliando um restaurante ou como avaliam um político. A forma mais precisa, porém mais ingênua, de fazer isso é pagar algumas pessoas para monitorar os comentários das mídias sociais. No entanto, milhares de comentários e avaliações são geradas por dia e usar pessoas para monitorá-los seria ou muito caro ou simplesmente impraticável. Uma alternativa mais razoável é usar processamento de linguagem natural (PNL) para automatizar o monitoramento dos sentimentos nos textos. Mas, mesmo em PNL, há várias formas de abordar esse tipo de tarefa.\n",
    "\n",
    "A forma mais simples consiste em primeiro construir um vocabulário de palavras que estão associadas a sentimentos positivos e negativos, tais como \"excelente\" e \"péssimo\", respectivamente. Geralmente, é preciso pagar algum bom linguista para essa primeira etapa. Em seguida, ao analisar um texto, nós simplesmente contamos o número de palavras negativas e positivas para conseguir uma pontuação. Se essa pontuação ultrapassar um limiar preestabelecido, nós então classificamos o texto como tendo um sentimento positivo. Essa metodologia funciona bem, mesmo sendo extremamente simples. No entanto, ela é extremamente dependente do domínio da linguagem analisada, uma vez que palavras que expressam sentimento positivo em um contexto podem corresponder a sentimentos negativos em outros. Por exemplo, a palavra “denso” pode indicar uma avaliação positiva de um livro, mas também pode indicar uma reação negativa a uma massa italiana.\n",
    "\n",
    "Uma segunda forma de realizar analise de sentimento é usar aprendizado de máquina. Em vez de pagar um linguista para determinar quais são as palavras negativas e positivas, nós coletamos milhares de exemplos de texto positivo e texto negativo e treinamos uma máquina para aprender por conta própria as palavras positivas e negativas. Achar milhares de texto com sentimento negativo e positivo é fácil. Basta baixar [bases avaliações de produtos da Amazon](http://jmcauley.ucsd.edu/data/amazon/), e supor que avaliações com mais de 3 estrelas são positivas e com menos de 3 estrelas, negativas (as com 3 estrelas nós descartamos como sendo neutras). Em seguida, para cada palavra no nosso vocabulário, nós contamos quantas vezes ela aparece nos textos com sentimento positivos e negativos; então dividimos o número de ocorrências nos textos positivos pelo número de ocorrências nos textos negativos para conseguir uma pontuação para cada palavra. Essa pontuação será proporcional a \"positividade\" da palavra. Com isso, é fácil conseguir uma pontuação para o texto: basta tirar a média das pontuações de cada palavra nele. Essa abordagem é extremamente eficiente, fácil de implementar e funciona super bem. Ela resolve a primeira etapa do processo de PNL, isto é, não é precismo mais pagar um linguista para construir léxicos negativos e positivos. No entanto, cabe ressaltar alguns problemas que esse método não resolve.\n",
    "\n",
    "Em primeiro lugar, ainda estamos desconsiderando informação do contexto, que pode dar um sentimento diferente para a mesma palavra. Em segundo lugar, ao simplesmente somar a pontuação das palavras para conseguir uma pontuação para o texto, estamos ignorando a estrutura sequencial do texto. Por exemplo, uma avaliação como \"Eu estava esperando algo realmente desastroso, mas no final acabei me surpreendendo\" tem duas orações, sendo que a primeira é negativa, mas a segunda é positiva. No português, o sentimento presente na oração coordenada adversativa predomina, então a frase como um todo é positiva. Isso é extremamente difícil de notar com um algoritmo de aprendizado que ignora a estrutura sequencial do texto. Uma outra dificuldade desse tipo de método é lidar com expressões idiomáticas, como \"amigo da onça\". Um algoritmo que analisasse cada palavra individualmente provavelmente atribuiria um sentimento positivo a essa expressão (por conta da palavra amigo), o que estaria equivocado.\n",
    "\n",
    "Para lidar com essas dificuldades, podemos usar redes neurais recorrentes. Elas funcionam de forma iterativa, lendo a o texto em sequência, palavra por palavra. No início da frase, a rede neural codifica a primeira palavra em uma representação interna; na próxima iteração, a rede neural observa tanto a segunda palavra quanto a representação interna da iteração anterior e assim produz uma nova representação interna. Esse processo é repetido até o final da sequência de palavras. Nossa esperança é que, palavra por palavra, a rede vá incorporando na sua representação interna a informação presente no texto. Então, por fim, a partir da representação interna na última iteração, a rede neural produz uma probabilidade da frase ter um sentimento positivo. Isso resolve os dois problemas citados acima. Primeiro, as redes neurais recorrentes são feitas para representar sequências, então o texto é facilmente interpretado dessa forma. Em segundo lugar, a capacidade de representar sequências dá a rede neural poder para entender expressões idiomáticas; ela pode simplesmente codificar em sua representação interna a sequência de palavras da expressão de forma distinta de como codificaria cada palavra separadamente. Muito bem, isso soa promissor. Vamos tentar\n",
    "\n",
    "## O Modelo\n",
    "Em primeiro lugar, baixei as [avaliações da Amazon](http://jmcauley.ucsd.edu/data/amazon/). Lá, cada comentário/avaliação era associado a uma classificação de 1 a 5 estrelas. Eu supus que todas as avaliações com mais de 3 estrelas eram positivas e todas com menos de 3, negativas. Depois, usando 2 milhões de avaliações (1m de positivas e 1m de negativas), treinei o seguinte modelo de rede neural recorrente.\n",
    "\n",
    "| Camada        | Formato                        | Especificação  |\n",
    "| :-----------: |:------------------------------:| :-------------:|\n",
    "| Entrada       | Sequencia, 1 variável          | Índicies das palavras |\n",
    "| Representação | Sequência, 64 variáveis        |   Representação das palavras |\n",
    "| Recorrente    | Sequência, 32 variáveis        |   LSTM bi-direcional|\n",
    "| Dropout       | Sequência, 32 variáveis        |   Destruição de 70% |\n",
    "| Recorrente    | Sequência, 32 variáveis        |   LSTM bi-direcional|\n",
    "| Dropout       | Sequência, 32 variáveis        |   Destruição de 70% |\n",
    "| Saída         | Escalar, 1 variável            |   Função logística  |\n",
    "\n",
    "Nessa rede neural, as duas primeiras camadas tomam conta de codificar as palavras do texto em uma representação matemática. Para mais informações sobre esse processo, sugiro um [post que fiz](https://matheusfacure.github.io/2017/03/20/word2vec/) há algum tempo. Em seguida, as duas camadas recorrentes leem o texto palavra por palavra (com as palavras já codificadas), construindo uma representação interna da sequência observada. Você pode pensar na rede neural codificando a sequência em uma representação interna abstrata. Por fim, nós pegamos essa codificação abstrata e passamos à uma camada de saída, cujo trabalho é decodificar a representação interna da rede neural em uma probabilidade - a probabilidade do texto conter um sentimento positivo.\n",
    "\n",
    "## O experimento\n",
    "\n",
    "Muito bonito o modelo acima, mas funciona? Para ver isso, eu reservei 5% das 2mi avaliações e treinei essa rede neural nos outros 95% dos dados. Depois de pouco mais de uma hora, o modelo conseguiu prever corretamente mais de 92% do sentimento das avaliações não utilizadas para o treino. Podemos dizer que esse resultado é satisfatório. Não é nada fenomenal, mas já algo que pode ser usado industrialmente. Um detalhe importante, é que testamos o modelo treinado com avaliações do site da Amazon e essas avaliações**já tem uma pontuação de sentimento atrelado a elas**. Em outras palavras, não tem sentido prático prever avaliações da Amazon, porque já podemos saber o sentimento delas. \n",
    "\n",
    "Para avaliar o modelo com exemplos mais reais, eu fui ao Facebook, em páginas como a do Prêmio Nobel, The Economist, The New York Times e TED Talks, e peguei alguns exemplos de comentários. Então, utilizei o modelo treinado para conseguir a probabilidade de sentimento positivo de cada um deles.\n",
    "\n",
    "Dos onze comentários com sentimento positivo que coletei, dois foram classificados de forma errada. O primeiro deles é o seguinte: \n",
    "```\n",
    "The far reaching extent of people in the world are inherently good. This man could have ran off, and done nothing. Instead, he stepped up. We need more people to step up in all phases of life. This isn't an issue we can kill our way out of.\n",
    "```\n",
    "A rede neural deu uma probabilidade de 0.35 desse comentário ser positivo. O outro comentário classificado de forma errada recebeu uma probabilidade de 0.18 de ser positivo:\n",
    "\n",
    "```\n",
    "Just because I'm homeless doesn't mean I haven't got a heart. You, sir, have more heart than many very extremely wealthy men and women whose jobs are to care for their fellow citizens... I'm from the USA so yes, I am referring to our cruel leadership in DC. I hope they read your story and take inspiration from your actions. Bless and thank you\n",
    "```\n",
    "Esse comentário diz respeito a atuação heroica de um sem teto no atentado de Manchester (05/2017). Podemos ver que ele carrega um sentimento misto de louvor à ação do homem, mas também expressa uma indignidade com como as lideranças dos Estados Unidos. Eu mesmo tive minhas dúvidas antes de julgar esse comentário como tendo um sentimento positivo; não é surpresa então que a rede neural também tenha se confundido.\n",
    "\n",
    "Alguns outros exemplos de comentários positivos, corretamente classificados (com mais de 95% de certeza), são os seguintes:\n",
    "\n",
    "```\n",
    "Congratulations Taiwan! This is a great day for human rights and equality before the law. As per usual, the homophobic religious nutjobs spewing their hateful bile on here are ALL closeted gays themselves who are trying to divert attention away from their own deeply repressed same sex attractions. It is classic Freudian psychology!\n",
    "```\n",
    "\n",
    "```\n",
    "When I told everyone I wanted to be a stand up comic they all laughed. Ten years later I finally made it and nobody's laughing now\n",
    "```\n",
    "\n",
    "```\n",
    "Congratulations Taiwan! This is a great day for human rights and equality before the law. As per usual, the homophobic religious nutjobs spewing their hateful bile on here are ALL closeted gays themselves who are trying to divert attention away from their own deeply repressed same sex attractions. It is classic Freudian psychology!\n",
    "```\n",
    "\n",
    "```\n",
    "Although I am an Atheist and although the Catholic Church has been implicit in horrible crimes against humanity and specifically the vulnerable, I believe that Pope Francis is good, honest and empathetic man and an excellent religious leader for these times.\n",
    "```\n",
    "\n",
    "Esse último exemplo tem uma particularidade. Primeiro, a pessoa expressa um sentimento negativo e em seguida, contrapõe o que tinha dito com um sentimento positivo, que predomina no comentário. Esse caso é análogo ao uso de orações coordenadas adversativas que discutimos acima e é particularmente difícil de ser classificado corretamente por modelos que desconsideram a ordem das palavras.\n",
    "\n",
    "Quanto aos exemplos de comentários negativos, dois dos doze que coletei foram incorretamente classificados.\n",
    "\n",
    "```\n",
    "I can't believe how this coward would choose a concert venue for a pop singer whose fan base demographic are teenage girls. Make no mistake about it, children were the deliberate targets for this latest horrific, terrorist attack. Parents unsuspectingly took their innocent teenage girls to an Ariana Grande concert for a seemingly fun night of entertainment and were victimized by a deadly suicide bombing. Even those who were not killed or injured will be mentally traumatized for life. Thank God the terrorist never made it inside of the venue and the bomb detonated outside the main entrance in a public space, or else the fatalities would have been much worse. Acts of violence against innocent children in the name of a religion or ideology? The dehumanization continues. God help us. Praying for the victims and their families at this time!\n",
    "```\n",
    "\n",
    "```\n",
    "My deepest condolences to the families. As a Muslim I feel ashamed that my religion is being used to commit crimes like these. I wish all those Muslims who have issues with Western values would just leave and pick a country who shares their values and has no secularism. I wonder how long they would make it there.\n",
    "```\n",
    "\n",
    "Ambos os comentários foram classificados como positivos, com uma probabilidade maior do que 95%. Eu não sei porque isso aconteceu. Particularmente, não consigo ver nada que possa causar dúvidas quanto ao sentimento negativo. Outros exemplos de comentários negativos, esses classificados corretamente, são os seguintes:\n",
    "\n",
    "\n",
    "```\n",
    "The entire argument is flawed, because in your analogy, the owners of big companies are the Elizabeth I of our times. They are the only ones who gain net benefit from breakthroughs of industrial engineering. Everyone else breaks even or sees inflation erode their relative earnings.\n",
    "```\n",
    "\n",
    "```\n",
    "have we learned nothing from the terminator? Must we all die so some psychos in white coats play god? It's bad enough we have d wave. Can we jus chill out with the self destruction for like 5 minutes?\n",
    "```\n",
    "\n",
    "```\n",
    "Absolutely boring. I know GGM has his fanboys and lackeys out there but the fact that Joyce and Proust never got this award, it proves or shows that you Swedes cannot read well. Ugh, that first paragraph or even the first sentence of Cien años de soledad, ugh, how hideously composed! I wonder if Flaubert would have dug this Márquez dude. Why read García Márquez when one can enjoy Borges, Casares, Filloy, Di Benedetto or Mujica Lainez?\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "How can any army which occupies a people be considered remotely moral? If that was the case, the British Empire would have been the most moral and virtuous entity the world has ever known. It is an absolute travesty that this is even considered a debate, and shows how little man has actually progressed.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd # para manipulação de dados\n",
    "import numpy as np # para computação numérica\n",
    "from tools import read_dataframe # para processamento de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função `read_dataframe` abaixo aceita os argumentos `neg` e `pos`, que recebem arquivos `.txt` em que cada linha é uma avaliação. Para construir esses arquivos, vá em http://jmcauley.ucsd.edu/data/amazon/, baixe algumas bases de avaliações (formato `.json`) e rode na linha de comando `python3 tools.py analiações.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>this is the best mounting tape i have ever use...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>i got this paper for my first grader to practi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>the laminator works well enough as advertised ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>avery is a brand i trust i have tried other bu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>i ve used these to print a few sheets of retur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  sentiment\n",
       "315   this is the best mounting tape i have ever use...          1\n",
       "1041  i got this paper for my first grader to practi...          0\n",
       "1517  the laminator works well enough as advertised ...          0\n",
       "500   avery is a brand i trust i have tried other bu...          1\n",
       "738   i ve used these to print a few sheets of retur...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = read_dataframe(n_neg=1000, n_pos=1000) # aumente o número de linhas para 1 milhão para melhores resultados\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proporção de positivos no treino: 0.4994949494949495\n",
      "Proporção de positivos no teste: 0.55\n",
      "Fromatos de treino (1980,) (1980,)\n",
      "Formatos de teste (20,) (20,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# separa dados em treino e teste\n",
    "train, test, _, _ = train_test_split(data, data['text'], test_size=0.01, random_state=42)\n",
    "\n",
    "# separa texto dos sentimentos\n",
    "X_train, X_test = train.values[:, 0], test.values[:, 0]\n",
    "y_train, y_test = train.values[:, 1], test.values[:, 1]\n",
    "print('Proporção de positivos no treino:', y_train.mean())\n",
    "print('Proporção de positivos no teste:', y_test.mean())\n",
    "print('Fromatos de treino', X_train.shape, y_train.shape)\n",
    "print('Formatos de teste', X_test.shape, y_test.shape)\n",
    "# del data # libera RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importa funções para redes neurais\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "\n",
    "# treina um tokenizador nos dados de treino\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# tokeniza os dados\n",
    "X_train = tokenizer.texts_to_sequences(X_train) \n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# corta ou adiciona zeros em sequências maiores de 100\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato de treino (1980, 100) (1980,)\n",
      "Formato de teste (20, 100) (20,)\n"
     ]
    }
   ],
   "source": [
    "print('Formato de treino', X_train.shape, y_train.shape)\n",
    "print('Formato de teste', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# monta o modelo de rede neural\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 64, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 20 samples\n",
      "Epoch 1/3\n",
      "13s - loss: 0.6875 - acc: 0.5755 - val_loss: 0.6349 - val_acc: 0.7500\n",
      "Epoch 2/3\n",
      "12s - loss: 0.6557 - acc: 0.6010 - val_loss: 0.6148 - val_acc: 0.8000\n",
      "Epoch 3/3\n",
      "11s - loss: 0.4294 - acc: 0.8301 - val_loss: 0.2099 - val_acc: 0.9500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f10d341f898>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# treina a rede neural\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=128, epochs=3, verbose=2, validation_split=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# previsão de teste\n",
    "y_hat = (model.predict(X_test, batch_size=1000) > .5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "test['pred'] = y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>i tried this product and liked it very much bu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>this tape does exactly what it claims to do yo...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>its holds wrapping paper in place extremely we...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>the white board part of it may be nice but no ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>it s so frustrating when the drawers fall off ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>avery has some of the best products and this i...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1532</th>\n",
       "      <td>i own the xp 310 epson printer not this model ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>this is suppose to assist you in making a smal...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>i was very happy with this paper my only issue...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>i like to use my laser printer to print labels...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>we bought these to use on our save the date ca...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>the main reason i like these labels 1 easy to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>this tape has an excellent adhesive sticking t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>speaking as a loyal panasonic phone owner for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>after having problems with the cheaper non oem...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>i use these labels to do marketing campaigns f...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>and it does what it is supposed to do i just ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714</th>\n",
       "      <td>the keyboard tray arrived at my house in a ver...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>avery always makes great labels and gives you ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>shortest review i ve probably ever written ins...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  sentiment  pred\n",
       "1029  i tried this product and liked it very much bu...          0     1\n",
       "956   this tape does exactly what it claims to do yo...          1     1\n",
       "944   its holds wrapping paper in place extremely we...          1     1\n",
       "1077  the white board part of it may be nice but no ...          0     0\n",
       "1300  it s so frustrating when the drawers fall off ...          0     1\n",
       "420   avery has some of the best products and this i...          1     1\n",
       "1532  i own the xp 310 epson printer not this model ...          0     0\n",
       "1604  this is suppose to assist you in making a smal...          0     1\n",
       "182   i was very happy with this paper my only issue...          1     1\n",
       "626   i like to use my laser printer to print labels...          1     1\n",
       "762   we bought these to use on our save the date ca...          1     1\n",
       "754   the main reason i like these labels 1 easy to ...          1     1\n",
       "883   this tape has an excellent adhesive sticking t...          1     1\n",
       "1490  speaking as a loyal panasonic phone owner for ...          0     1\n",
       "1195  after having problems with the cheaper non oem...          0     0\n",
       "719   i use these labels to do marketing campaigns f...          1     1\n",
       "849    and it does what it is supposed to do i just ...          1     1\n",
       "1714  the keyboard tray arrived at my house in a ver...          0     0\n",
       "757   avery always makes great labels and gives you ...          1     1\n",
       "1599  shortest review i ve probably ever written ins...          0     0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mostra a acurácia de teste\n",
    "print((test['pred'] == test['sentiment']).mean())\n",
    "test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My deepest condolences to the families. As a M...</td>\n",
       "      <td>0.852564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I can't believe how this coward would choose a...</td>\n",
       "      <td>0.182854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yeah there's a lot of waste in the budget. App...</td>\n",
       "      <td>0.636512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>These photos are silly. Do not play with peopl...</td>\n",
       "      <td>0.573776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Absolutely boring. I know GGM has his fanboys ...</td>\n",
       "      <td>0.160405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Einstein was a genius and he came up with grea...</td>\n",
       "      <td>0.631116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How can any army which occupies a people be co...</td>\n",
       "      <td>0.220767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Yeah, this has to be the most useless post I'v...</td>\n",
       "      <td>0.205866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Just a really weak article. No insights whatso...</td>\n",
       "      <td>0.693893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The entire argument is flawed, because in your...</td>\n",
       "      <td>0.681964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>have we learned nothing from the terminator? M...</td>\n",
       "      <td>0.860298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ashamed how people that follow TED are becomin...</td>\n",
       "      <td>0.658772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0      pred\n",
       "0   My deepest condolences to the families. As a M...  0.852564\n",
       "1   I can't believe how this coward would choose a...  0.182854\n",
       "2   Yeah there's a lot of waste in the budget. App...  0.636512\n",
       "3   These photos are silly. Do not play with peopl...  0.573776\n",
       "4   Absolutely boring. I know GGM has his fanboys ...  0.160405\n",
       "5   Einstein was a genius and he came up with grea...  0.631116\n",
       "6   How can any army which occupies a people be co...  0.220767\n",
       "7   Yeah, this has to be the most useless post I'v...  0.205866\n",
       "8   Just a really weak article. No insights whatso...  0.693893\n",
       "9   The entire argument is flawed, because in your...  0.681964\n",
       "10  have we learned nothing from the terminator? M...  0.860298\n",
       "11  Ashamed how people that follow TED are becomin...  0.658772"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_txt = pd.read_table('neg_text.txt', header=None)\n",
    "X_neg_test = tokenizer.texts_to_sequences(neg_txt.ix[:, 0].values)\n",
    "X_neg_test = sequence.pad_sequences(X_neg_test, maxlen=maxlen)\n",
    "neg_txt['pred'] = model.predict(X_neg_test, batch_size=10)\n",
    "neg_txt.to_csv('test_resp_neg.csv')\n",
    "neg_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vry good article, it exposes the excesses and ...</td>\n",
       "      <td>0.837182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My daughter loves the Uni book. She's fascinat...</td>\n",
       "      <td>0.812413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When I told everyone I wanted to be a stand up...</td>\n",
       "      <td>0.885709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The far reaching extent of people in the world...</td>\n",
       "      <td>0.729148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just because I'm homeless doesn't mean I haven...</td>\n",
       "      <td>0.886124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I don't really understand all the controversy ...</td>\n",
       "      <td>0.661833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OMG, this is one of my absolute favorite novel...</td>\n",
       "      <td>0.188376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Inconsistent social prestige is what he is des...</td>\n",
       "      <td>0.780331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Congratulations Taiwan! This is a great day fo...</td>\n",
       "      <td>0.874758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This is incredibly fascinating to me. I've see...</td>\n",
       "      <td>0.040433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Although I am an Atheist and although the Cath...</td>\n",
       "      <td>0.894284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0      pred\n",
       "0   Vry good article, it exposes the excesses and ...  0.837182\n",
       "1   My daughter loves the Uni book. She's fascinat...  0.812413\n",
       "2   When I told everyone I wanted to be a stand up...  0.885709\n",
       "3   The far reaching extent of people in the world...  0.729148\n",
       "4   Just because I'm homeless doesn't mean I haven...  0.886124\n",
       "5   I don't really understand all the controversy ...  0.661833\n",
       "6   OMG, this is one of my absolute favorite novel...  0.188376\n",
       "7   Inconsistent social prestige is what he is des...  0.780331\n",
       "8   Congratulations Taiwan! This is a great day fo...  0.874758\n",
       "9   This is incredibly fascinating to me. I've see...  0.040433\n",
       "10  Although I am an Atheist and although the Cath...  0.894284"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_txt = pd.read_table('pos_text.txt', header=None)\n",
    "X_pos_test = tokenizer.texts_to_sequences(pos_txt.ix[:, 0].values)\n",
    "X_pos_test = sequence.pad_sequences(X_pos_test, maxlen=maxlen)\n",
    "pos_txt['pred'] = model.predict(X_pos_test, batch_size=10)\n",
    "pos_txt.to_csv('res_test_pos.csv')\n",
    "pos_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
